{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate agent text based responses, to Likert scale scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# delete as applicable:\n",
    "population_label = 'popc'\n",
    "# population_label = 'popp'\n",
    "\n",
    "parts = []\n",
    "for i in range(6):\n",
    "    parts.append(pd.read_csv(f'{population_label}_responses_{i+1}.csv', index_col=0))\n",
    "    \n",
    "df_responses = pd.concat(parts)\n",
    "df_responses[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses file is too big for GitHub, break it up into smaller parts.\n",
    "break_file_up = False\n",
    "if break_file_up:\n",
    "    df_responses[:50].to_csv(f'{population_label}_responses_1.csv')\n",
    "    df_responses[50:100].to_csv(f'{population_label}_responses_2.csv')\n",
    "    df_responses[100:150].to_csv(f'{population_label}_responses_3.csv')\n",
    "    df_responses[150:200].to_csv(f'{population_label}_responses_4.csv')\n",
    "    df_responses[200:250].to_csv(f'{population_label}_responses_5.csv')\n",
    "    df_responses[250:].to_csv(f'{population_label}_responses_6.csv')\n",
    "\n",
    "    parts = []\n",
    "    for i in range(6):\n",
    "        parts.append(pd.read_csv(f'{population_label}_responses_{i+1}.csv', index_col=0))\n",
    "        \n",
    "    combined = pd.concat(parts)\n",
    "    combined.to_csv(f'{population_label}_responses_TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_responses(df):\n",
    "    error_count = 0\n",
    "    specific_error_count = 0\n",
    "    \n",
    "    for agent in df.index:\n",
    "        for adj in df.columns:\n",
    "            answer = df.loc[agent, adj]\n",
    "            if answer == '[error]':\n",
    "                print(f\"Error: {agent}, found '[error]' in results for {adj}.\")\n",
    "                error_count += 1\n",
    "                if adj == \"niggardly\":\n",
    "                    specific_error_count += 1\n",
    "                \n",
    "    return error_count, specific_error_count\n",
    "\n",
    "check_responses(df_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "\n",
    "for adj in list(df_responses.columns):\n",
    "    values = df_responses[adj].value_counts()\n",
    "    if '[error]' in values:\n",
    "        errors[adj] = values['[error]']\n",
    "        \n",
    "[print(f\"'{adj}' {count}\") for adj, count in errors.items() if count > 1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# the 9-point scale.\n",
    "expected_answers = ['Extremely Inaccurate',\n",
    "                    'Very Inaccurate',\n",
    "                    'Moderately Inaccurate',\n",
    "                    'Slightly Inaccurate',\n",
    "                    'Neither Accurate Nor Inaccurate',\n",
    "                    'Slightly Accurate',\n",
    "                    'Moderately Accurate',\n",
    "                    'Very Accurate',\n",
    "                    'Extremely Accurate']\n",
    "\n",
    "def match_accuracy(text):\n",
    "    pattern = '|'.join(re.escape(level) for level in expected_answers)\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group().lower()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def translate_response(answer, error_value=5):\n",
    "    ret = match_accuracy(answer)\n",
    "    if ret == 'Extremely Inaccurate'.lower():\n",
    "        return 1\n",
    "    elif ret == 'Very Inaccurate'.lower():\n",
    "        return 2\n",
    "    elif ret == 'Moderately Inaccurate'.lower():\n",
    "        return 3\n",
    "    elif ret == 'Slightly Inaccurate'.lower():\n",
    "        return 4\n",
    "    elif ret == 'Neither Accurate Nor Inaccurate'.lower():\n",
    "        return 5\n",
    "    elif ret == 'Slightly Accurate'.lower():\n",
    "        return 6\n",
    "    elif ret == 'Moderately Accurate'.lower():\n",
    "        return 7\n",
    "    elif ret == 'Very Accurate'.lower():\n",
    "        return 8\n",
    "    elif ret == 'Extremely Accurate'.lower():\n",
    "        return 9\n",
    "    return error_value\n",
    "\n",
    "df_scores = df_responses.map(translate_response)\n",
    "df_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(f'{population_label}_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
