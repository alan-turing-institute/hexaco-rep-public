{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate agent text based responses, to Likert scale scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# delete as applicable:\n",
    "# population_label = 'popc'\n",
    "population_label = 'popp'\n",
    "\n",
    "parts = []\n",
    "for i in range(6):\n",
    "    parts.append(pd.read_csv(f'{population_label}_responses_{i+1}.csv', index_col=0))\n",
    "    \n",
    "df_responses = pd.concat(parts)\n",
    "df_responses[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses file is too big for GitHub, break it up into smaller parts.\n",
    "break_file_up = False\n",
    "if break_file_up:\n",
    "    df_responses[:50].to_csv(f'{population_label}_responses_1.csv')\n",
    "    df_responses[50:100].to_csv(f'{population_label}_responses_2.csv')\n",
    "    df_responses[100:150].to_csv(f'{population_label}_responses_3.csv')\n",
    "    df_responses[150:200].to_csv(f'{population_label}_responses_4.csv')\n",
    "    df_responses[200:250].to_csv(f'{population_label}_responses_5.csv')\n",
    "    df_responses[250:].to_csv(f'{population_label}_responses_6.csv')\n",
    "\n",
    "    parts = []\n",
    "    for i in range(6):\n",
    "        parts.append(pd.read_csv(f'{population_label}_responses_{i+1}.csv', index_col=0))\n",
    "        \n",
    "    combined = pd.concat(parts)\n",
    "    combined.to_csv(f'{population_label}_responses_TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_responses(df):\n",
    "    error_count = 0\n",
    "    specific_error_count = 0\n",
    "    \n",
    "    for agent in df.index:\n",
    "        for adj in df.columns:\n",
    "            answer = df.loc[agent, adj]\n",
    "            if answer == '[error]':\n",
    "                print(f\"Error: {agent}, found '[error]' in results for {adj}.\")\n",
    "                error_count += 1\n",
    "                if adj == \"niggardly\":\n",
    "                    specific_error_count += 1\n",
    "                \n",
    "    return error_count, specific_error_count\n",
    "\n",
    "check_responses(df_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "\n",
    "for adj in list(df_responses.columns):\n",
    "    values = df_responses[adj].value_counts()\n",
    "    if '[error]' in values:\n",
    "        errors[adj] = values['[error]']\n",
    "        \n",
    "[print(f\"'{adj}' {count}\") for adj, count in errors.items() if count > 1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# the 9-point scale.\n",
    "expected_answers = ['Extremely Inaccurate',\n",
    "                    'Very Inaccurate',\n",
    "                    'Moderately Inaccurate',\n",
    "                    'Slightly Inaccurate',\n",
    "                    'Neither Accurate Nor Inaccurate',\n",
    "                    'Slightly Accurate',\n",
    "                    'Moderately Accurate',\n",
    "                    'Very Accurate',\n",
    "                    'Extremely Accurate']\n",
    "\n",
    "def match_accuracy(text):\n",
    "    pattern = '|'.join(re.escape(level) for level in expected_answers)\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group().lower()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def translate_response(answer, error_value=5):\n",
    "    ret = match_accuracy(answer)\n",
    "    if ret == 'Extremely Inaccurate'.lower():\n",
    "        return 1\n",
    "    elif ret == 'Very Inaccurate'.lower():\n",
    "        return 2\n",
    "    elif ret == 'Moderately Inaccurate'.lower():\n",
    "        return 3\n",
    "    elif ret == 'Slightly Inaccurate'.lower():\n",
    "        return 4\n",
    "    elif ret == 'Neither Accurate Nor Inaccurate'.lower():\n",
    "        return 5\n",
    "    elif ret == 'Slightly Accurate'.lower():\n",
    "        return 6\n",
    "    elif ret == 'Moderately Accurate'.lower():\n",
    "        return 7\n",
    "    elif ret == 'Very Accurate'.lower():\n",
    "        return 8\n",
    "    elif ret == 'Extremely Accurate'.lower():\n",
    "        return 9\n",
    "    return error_value\n",
    "\n",
    "df_scores = df_responses.map(translate_response)\n",
    "df_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(f'{population_label}_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipsatise_scores(df):\n",
    "    # drop 'niggardly' due to high content filter hit rate.\n",
    "    del df['niggardly']\n",
    "    \n",
    "    # drop any other adjective that has stdev of 0.\n",
    "    to_drop = df.columns[df.std()==0]\n",
    "    print(f'Dropping {len(to_drop)} adjectives (stdev==0): '+', '.join(to_drop))\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    print(f'shape = {df.shape}')\n",
    "    \n",
    "    # ipsatise, as follows:\n",
    "    df_mean = df.mean()\n",
    "    \n",
    "    # (a) his/her mean self-ratings across adjectives where \n",
    "    # the mean self-rating score (of everyone's response) \n",
    "    # is less than 5\n",
    "    df_negs = df[df_mean[df_mean < 5].index]\n",
    "    print(f\"df_negs shape = {df_negs.shape}\")\n",
    "    \n",
    "    # (b) his/her mean self-ratings across adjectives where \n",
    "    # the mean self-rating score (of everyone's response) \n",
    "    # is *greater than or equal to* 5\n",
    "    df_pos = df[df_mean[df_mean >= 5].index]\n",
    "    print(f\"df_pos shape = {df_pos.shape}\")\n",
    "\n",
    "    df_self_rating_means = pd.DataFrame({'positive': df_pos.mean(axis=1), 'negative': df_negs.mean(axis=1)})\n",
    "    df_self_rating_means['average'] = df_self_rating_means.mean(axis=1) \n",
    "    print(f\"df_self_rating_means shape = {df_self_rating_means.shape}\")\n",
    "\n",
    "    # now make the ipsatized data frame... 'this involves: \n",
    "    # for each adjective, subtracting the participantâ€™s mean \n",
    "    # self-rating across all adjectives from his or her self-rating \n",
    "    # on the adjective in question and then dividing this \n",
    "    # difference by his or her standard deviation of self-ratings \n",
    "    # across all adjectives'\n",
    "    adjusted_mean = df_self_rating_means['average']\n",
    "\n",
    "    stdevs = df.std(axis=1)\n",
    "    ipsatized_data = (df.sub(adjusted_mean, axis=0)).div(stdevs, axis=0)\n",
    "    print(f\"ipsatized_data shape = {ipsatized_data.shape}\")\n",
    "    return ipsatized_data\n",
    "\n",
    "df_ipsatised = ipsatise_scores(df_scores)\n",
    "df_ipsatised.to_csv(f'{population_label}_ipsatised_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
